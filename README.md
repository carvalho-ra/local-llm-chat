# ğŸš€ Local AI Chat with Ollama and Streamlit

## ğŸ“‹ Description

This project creates a simple web-based chat interface using Streamlit that interacts with a local AI language model served by Ollama. It allows users to send messages and receive AI-generated responses, maintaining conversation history across user inputs.

## ğŸ› ï¸ Technologies Used

- Python 3  
- Streamlit â€” for building the web interface  
- Ollama â€” for running local LLM models like Llama 3  
- Requests â€” for making HTTP POST requests to the local Ollama API  

## âš™ï¸ How It Works

- ğŸ–¥ï¸ Starts a local Ollama server with a downloaded LLM model (e.g., Llama 3).  
- ğŸŒ Provides a web interface where users can type messages.  
- ğŸ” Maintains conversation history using Streamlit session state.  
- ğŸ“¡ Sends the full conversation as context to the Ollama API via HTTP POST.  
- ğŸ¤– Displays AI-generated responses in the chat interface.

## âœ… Setup Instructions

### ğŸ“Œ Prerequisites

- Python 3 installed on your system  
- Ollama installed and running locally  

### ğŸ§± Environment Setup

1. Install Ollama:

```bash
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama3
ollama serve
```

2. Create and activate a Python virtual environment:
```bash
python3 -m venv venv
source venv/bin/activate    # Linux / MacOS
venv\Scripts\activate       # Windows
```

3. Install Python dependencies:
```bash
pip install -r requirements.txt
```

## â–¶ï¸ Running the App
```bash
streamlit run main.py
```

## ğŸŒ Access

Open your browser at:
```bash
http://localhost:8501
```

### âš ï¸ Important Notes

- Ollama must be running locally and serving a model (e.g., llama3) at http://localhost:11434.
- This app uses a simple prompt-building strategy by sending the entire conversation history as context to the model.